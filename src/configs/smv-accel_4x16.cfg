#######################
# Common configurations
#######################
pipelining,0
cycle_time,1
ready_mode,0

#######################
# Array partitions
#######################

# NUM_PE=4 NUM_MACCS=2 (NUM_MACCS means how many 8-wide vectors are used)
# I modify convolution and matrix multiply partitions

# SMV convolution
# scratchpads: 65KB, words are 4 bytes (32bit), partitioned by PEs (I guess)
partition,cyclic,inputs,65536,4,4
partition,cyclic,weights,65536,4,4
partition,cyclic,results,65536,4,4
# registers: 4 dimensions arrays, int 4 is 4 bytes (4*4=16), words are 4 bytes 
partition,complete,inputs_dims,16,4
partition,complete,weights_dims,16,4
partition,complete,results_dims,16,4
partition,complete,inputs_halo_pad,16,4
# act params comes from out, I guess 5 dims, could check the code
partition,complete,act_params,20,4
# registers inside the PEs group
# kernel_reg is v8fp_t*NUM_PE*NUM_MACC=32*4*2=256
partition,complete,kernel_reg,256,4
# result_buffer is v8fp_t, for 8PEs, underused with less PEs, this can't change 
partition,complete,results_buffer,32,4
# smv_conv_product_reg v8fp_t*NUM_PE*NUM_MACC=32*4*2=256
partition,complete,smv_conv_product_reg,256,4
# act_reg v8fp_t*NUM_MACC=32*2=64
partition,complete,act_reg,64,4
# accum_vec_reg v8fp_t*NUM_PE=32*4=128
partition,complete,accum_vec_reg,128,4
# accum_reg float*NUM_PE=4*4=16
partition,complete,accum_reg,16,4

# SMV matrix multiply: changed from SMAUG's default for full parallelism; 
# I have followed the convolution code idea
# scratchpads: 65KB, words are 4 bytes (32bit), partitioned by PEs (I guess)
partition,cyclic,a,65536,4,4
partition,cyclic,b,65536,4,4
partition,cyclic,results,65536,4,4
# a_dims, b_dims, 2 dimensions -> 2*4byte = 8 byte register
partition,complete,a_dims,8,4
partition,complete,b_dims,8,4
partition,complete,results_dims,8,4
# registers inside the PEs group
# b_reg is v8fp_t*NUM_PE*NUM_MACC=32*4*2=256, rows of b into pe
partition,complete,b_reg,256,4
# partial_sums is v8fp_t, same as result buffer, underused with less PEs
partition,complete,partial_sums,32,4
# product_reg is v8fp_t*NUM_PE*NUM_MACC=32*4*2=256, a row * b rows
partition,complete,product_reg,256,4
# a_reg is v8fp_t, 8 values of the row of a 
partition,complete,a_reg,32,4
# accum_vec_reg v8fp_t*NUM_PE=32*4=128
partition,complete,accum_vec_reg,128,4
# accum_reg float*NUM_PE=4*4=16
partition,complete,accum_reg,16,4

# SMV max/avg pooling
partition,complete,curr_results,32,4

# SMV batch norm
partition,complete,mean_vec,32,4
partition,complete,recip_sqrt_var_vec,32,4
partition,complete,gamma_vec,32,4
partition,complete,beta_vec,32,4

# Elementwise addition
partition,cyclic,inputs0,65536,4,8
partition,cyclic,inputs1,65536,4,8
partition,cyclic,results,65536,4,8

# Other
partition,complete,sampling,8,4

#######################
# Loop configurations
#######################

# I change only the matrix multiply

# SMV convolution
unrolling,smv_conv3d_nhwc_vec_fxp,ofmap_block_iteration,1
unrolling,smv_conv3d_nhwc_vec_fxp,k_col,1
unrolling,smv_conv3d_nhwc_vec_fxp,k_row,1
unrolling,smv_conv3d_nhwc_vec_fxp,pe_iteration,1
flatten,smv_conv3d_nhwc_vec_fxp,load_kern_pe
flatten,smv_conv3d_nhwc_vec_fxp,load_kern_mu
unrolling,smv_conv3d_nhwc_vec_fxp,conv3d_col,1
unrolling,smv_conv3d_nhwc_vec_fxp,conv3d_row,1
#pipeline,smv_conv3d_nhwc_vec_fxp,conv3d_row
pipeline,smv_conv3d_nhwc_vec_fxp,conv3d_col
flatten,smv_conv3d_nhwc_vec_fxp,load_act_mu
flatten,smv_conv3d_nhwc_vec_fxp,pe_groups
flatten,smv_conv3d_nhwc_vec_fxp,mu_groups
flatten,smv_conv3d_nhwc_vec_fxp,reduction_1
flatten,smv_conv3d_nhwc_vec_fxp,reduction_2

# SMV matrix multiply: changed from SMAUG's to achieve full parallelism;
# I unroll the pe calculations by NUM_PE and pipeline it
# I'm not sure if it is the best way, but seems to work well
unrolling,smv_matrix_multiply_transpose_nc_vec_fxp,a_act,1
unrolling,smv_matrix_multiply_transpose_nc_vec_fxp,b_col,1
unrolling,smv_matrix_multiply_transpose_nc_vec_fxp,b_row,1
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,a_reg_load
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,load_b_pe
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,load_b_mu
unrolling,smv_matrix_multiply_transpose_nc_vec_fxp,pe_insts,4
pipeline,smv_matrix_multiply_transpose_nc_vec_fxp,pe_insts
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,core_mul
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,reduce_1
flatten,smv_matrix_multiply_transpose_nc_vec_fxp,reduce_2

# SMV max pooling
unrolling,smv_maxpooling_nhwc_vec_fxp,maxpool_input_row,1
unrolling,smv_maxpooling_nhwc_vec_fxp,maxpool_input_col,1
unrolling,smv_maxpooling_nhwc_vec_fxp,maxpool_chan_grp,4
unrolling,smv_maxpooling_nhwc_vec_fxp,maxpool_pool_row,1
unrolling,smv_maxpooling_nhwc_vec_fxp,maxpool_pool_col,1
pipeline,smv_maxpooling_nhwc_vec_fxp,maxpool_pool_row
pipeline,smv_maxpooling_nhwc_vec_fxp,maxpool_pool_col
flatten,smv_maxpooling_nhwc_vec_fxp,maxpool_compare

# SMV avg pooling
unrolling,smv_avgpooling_nhwc_vec_fxp,avgpool_input_row,1
unrolling,smv_avgpooling_nhwc_vec_fxp,avgpool_input_col,1
unrolling,smv_avgpooling_nhwc_vec_fxp,avgpool_chan_grp,4
unrolling,smv_avgpooling_nhwc_vec_fxp,avgpool_pool_row,1
unrolling,smv_avgpooling_nhwc_vec_fxp,avgpool_pool_col,1
pipeline,smv_avgpooling_nhwc_vec_fxp,avgpool_pool_row
pipeline,smv_avgpooling_nhwc_vec_fxp,avgpool_pool_col

# SMV batch norm (post-conv NCHW)
unrolling,smv_batch_norm_post_conv_nchw_vec_fxp,bn_batch,1
unrolling,smv_batch_norm_post_conv_nchw_vec_fxp,bn_chan,1
unrolling,smv_batch_norm_post_conv_nchw_vec_fxp,bn_chan_vec,1
unrolling,smv_batch_norm_post_conv_nchw_vec_fxp,bn_row,1
unrolling,smv_batch_norm_post_conv_nchw_vec_fxp,bn_col,8
pipeline,smv_batch_norm_post_conv_nchw_vec_fxp,bn_col

# SMV batch norm (post-conv NHWC)
unrolling,smv_batch_norm_post_conv_nhwc_vec_fxp,bn_batch,1
unrolling,smv_batch_norm_post_conv_nhwc_vec_fxp,bn_chan,8
unrolling,smv_batch_norm_post_conv_nhwc_vec_fxp,bn_row,1
unrolling,smv_batch_norm_post_conv_nhwc_vec_fxp,bn_col,1

# SMV batch norm (post-fc)
pipeline,smv_batch_norm_post_fc_nc_vec_fxp,bn_input
unrolling,smv_batch_norm_post_fc_nc_vec_fxp,bn_batch,1
unrolling,smv_batch_norm_post_fc_nc_vec_fxp,bn_input,1

# SMV elementwise addition
unrolling,smv_eltwise_add_nc_vec_fxp,eltwise_add_loop,8

# Activation functions
unrolling,relu_vec,relu_loop,8
unrolling,lrelu_vec,lrelu_loop,8
unrolling,elu_vec,elu_loop,8
flatten,elu_vec_unit,elu_unit_loop
unrolling,selu_vec,selu_loop,8
flatten,selu_vec_unit,selu_unit_loop
unrolling,sigmoid_vec,sigmoid_loop,8
flatten,sigmoid_vec_unit,sigmoid_unit_loop
unrolling,tanh_vec,tanh_loop,8
unrolling,hard_tanh_vec,hard_tanh_loop,8
flatten,hard_tanh_vec_unit,hard_tanh_unit_loop

# FP16 load and store
unrolling,host_load_fp16,host_fp16_to_fp32,1
unrolling,host_load_fp16,vector_fp16_to_fp32,8
pipeline,host_load_fp16,host_fp16_to_fp32
pipeline,host_load_fp16,vector_fp16_to_fp32
unrolling,host_store_fp16,host_fp32_to_fp16,1
unrolling,host_store_fp16,vector_fp32_to_fp16,8
pipeline,host_store_fp16,host_fp32_to_fp16
pipeline,host_store_fp16,vector_fp32_to_fp16

